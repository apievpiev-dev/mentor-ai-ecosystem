#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∞ —Å–µ—Ä–≤–µ—Ä
"""

import asyncio
import json
import logging
import subprocess
import time
import requests
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

class FreeLocalAISystem:
    """–°–∏—Å—Ç–µ–º–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.available_models = {}
        self.installed_models = {}
        self.model_providers = {
            "ollama": self._setup_ollama,
            "huggingface": self._setup_huggingface,
            "transformers": self._setup_transformers
        }
        self._setup_directories()
    
    def _setup_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –º–æ–¥–µ–ª–µ–π"""
        directories = [
            "/workspace/free_models",
            "/workspace/free_models/ollama",
            "/workspace/free_models/huggingface",
            "/workspace/free_models/transformers",
            "/workspace/free_models/cache"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    async def _setup_ollama(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ Ollama
            result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
            if result.returncode != 0:
                logger.info("üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama...")
                await self._install_ollama()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º Ollama —Å–µ—Ä–≤–µ—Ä
            await self._start_ollama_server()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏
            free_models = [
                "llama3.1:8b",           # Meta Llama 3.1 8B
                "llama3.1:70b",          # Meta Llama 3.1 70B
                "codellama:latest",      # Code Llama
                "mistral:latest",        # Mistral 7B
                "neural-chat:latest",    # Intel Neural Chat
                "starling-lm:latest",    # Starling LM
                "phi3:latest",           # Microsoft Phi-3
                "gemma:latest",          # Google Gemma
                "qwen:latest",           # Alibaba Qwen
                "deepseek-coder:latest", # DeepSeek Coder
                "tinyllama:latest",      # TinyLlama (–æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∞—è)
                "orca-mini:latest"       # Orca Mini
            ]
            
            for model in free_models:
                await self._install_ollama_model(model)
            
            logger.info("‚úÖ Ollama –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama: {e}")
            return False
    
    async def _install_ollama(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama"""
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama
            install_script = """
            curl -fsSL https://ollama.ai/install.sh | sh
            """
            
            process = subprocess.Popen(
                install_script,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate()
            
            if process.returncode == 0:
                logger.info("‚úÖ Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {stderr}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {e}")
    
    async def _start_ollama_server(self):
        """–ó–∞–ø—É—Å–∫ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    logger.info("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
                    return
            except:
                pass
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä –≤ —Ñ–æ–Ω–µ
            process = subprocess.Popen(
                ['ollama', 'serve'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞
            for i in range(30):  # –ñ–¥–µ–º –¥–æ 30 —Å–µ–∫—É–Ω–¥
                try:
                    response = requests.get("http://localhost:11434/api/tags", timeout=2)
                    if response.status_code == 200:
                        logger.info("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")
                        return
                except:
                    await asyncio.sleep(1)
            
            logger.warning("‚ö†Ô∏è Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞: {e}")
    
    async def _install_ollama_model(self, model_name: str):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ Ollama"""
        try:
            logger.info(f"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –º–æ–¥–µ–ª—å {model_name}...")
            
            process = subprocess.Popen(
                ['ollama', 'pull', model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            while process.poll() is None:
                await asyncio.sleep(1)
            
            if process.returncode == 0:
                self.installed_models[model_name] = {
                    "provider": "ollama",
                    "status": "installed",
                    "installed_at": datetime.now().isoformat()
                }
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
    
    async def _setup_huggingface(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Hugging Face —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
        try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º transformers
            subprocess.run(['pip', 'install', 'transformers', 'torch', 'accelerate'], check=True)
            
            # –°–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Hugging Face
            free_models = [
                "microsoft/DialoGPT-medium",
                "distilbert-base-uncased",
                "bert-base-uncased",
                "gpt2",
                "facebook/blenderbot-400M-distill",
                "microsoft/DialoGPT-small",
                "t5-small",
                "google/flan-t5-small"
            ]
            
            for model in free_models:
                await self._cache_huggingface_model(model)
            
            logger.info("‚úÖ Hugging Face –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Hugging Face: {e}")
            return False
    
    async def _cache_huggingface_model(self, model_name: str):
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Hugging Face"""
        try:
            from transformers import AutoTokenizer, AutoModel
            
            logger.info(f"üì• –ö—ç—à–∏—Ä—É—é –º–æ–¥–µ–ª—å {model_name}...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="/workspace/free_models/huggingface")
            model = AutoModel.from_pretrained(model_name, cache_dir="/workspace/free_models/huggingface")
            
            self.installed_models[model_name] = {
                "provider": "huggingface",
                "status": "cached",
                "cached_at": datetime.now().isoformat()
            }
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∞")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
    
    async def _setup_transformers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            simple_models = {
                "simple_classifier": {
                    "type": "classification",
                    "description": "–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä",
                    "size": "small"
                },
                "simple_generator": {
                    "type": "generation", 
                    "description": "–ü—Ä–æ—Å—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞",
                    "size": "small"
                }
            }
            
            for model_name, config in simple_models.items():
                self.installed_models[model_name] = {
                    "provider": "transformers",
                    "status": "available",
                    "config": config,
                    "created_at": datetime.now().isoformat()
                }
            
            logger.info("‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤: {e}")
            return False
    
    async def setup_all_providers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π...")
        
        results = {}
        
        for provider_name, setup_func in self.model_providers.items():
            logger.info(f"üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ {provider_name}...")
            try:
                result = await setup_func()
                results[provider_name] = result
                if result:
                    logger.info(f"‚úÖ {provider_name} –Ω–∞—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                else:
                    logger.warning(f"‚ö†Ô∏è {provider_name} –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –æ—à–∏–±–∫–∞–º–∏")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ {provider_name}: {e}")
                results[provider_name] = False
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        await self._save_configuration()
        
        return results
    
    async def _save_configuration(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            config = {
                "installed_models": self.installed_models,
                "available_providers": list(self.model_providers.keys()),
                "setup_time": datetime.now().isoformat()
            }
            
            config_path = "/workspace/free_models/configuration.json"
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üíæ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    
    async def generate_response(self, prompt: str, model_name: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é –±–µ—Å–ø–ª–∞—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            if not model_name:
                # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
                model_name = await self._select_best_model()
            
            if model_name not in self.installed_models:
                return "–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
            
            model_info = self.installed_models[model_name]
            provider = model_info["provider"]
            
            if provider == "ollama":
                return await self._generate_with_ollama(prompt, model_name)
            elif provider == "huggingface":
                return await self._generate_with_huggingface(prompt, model_name)
            elif provider == "transformers":
                return await self._generate_with_transformers(prompt, model_name)
            else:
                return "–û—à–∏–±–∫–∞: –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä"
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return f"–û—à–∏–±–∫–∞: {str(e)}"
    
    async def _select_best_model(self) -> str:
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: Ollama > HuggingFace > Transformers
        for provider in ["ollama", "huggingface", "transformers"]:
            for model_name, model_info in self.installed_models.items():
                if model_info["provider"] == provider and model_info["status"] in ["installed", "cached", "available"]:
                    return model_name
        
        return "simple_classifier"  # Fallback
    
    async def _generate_with_ollama(self, prompt: str, model_name: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Ollama"""
        try:
            data = {
                "model": model_name,
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞")
            else:
                return f"–û—à–∏–±–∫–∞ Ollama: {response.status_code}"
                
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ Ollama: {str(e)}"
    
    async def _generate_with_huggingface(self, prompt: str, model_name: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Hugging Face"""
        try:
            from transformers import pipeline
            
            # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
            generator = pipeline("text-generation", model=model_name, cache_dir="/workspace/free_models/huggingface")
            
            result = generator(prompt, max_length=100, num_return_sequences=1)
            return result[0]["generated_text"]
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ Hugging Face: {str(e)}"
    
    async def _generate_with_transformers(self, prompt: str, model_name: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            if "classifier" in model_name:
                return f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {prompt[:50]}... (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é)"
            elif "generator" in model_name:
                return f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ: {prompt[:30]}... (–ª–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è)"
            else:
                return f"–û—Ç–≤–µ—Ç –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {prompt[:50]}..."
                
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤: {str(e)}"
    
    async def get_available_models(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return {
            "installed_models": self.installed_models,
            "total_models": len(self.installed_models),
            "providers": list(self.model_providers.keys())
        }
    
    async def get_model_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            status = {
                "ollama": False,
                "huggingface": False,
                "transformers": True,  # –í—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã
                "total_models": len(self.installed_models)
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                status["ollama"] = response.status_code == 200
            except:
                pass
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º Hugging Face
            try:
                import transformers
                status["huggingface"] = True
            except:
                pass
            
            return status
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {e}")
            return {"error": str(e)}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∏—Å—Ç–µ–º—ã
free_ai_system = FreeLocalAISystem()

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π...")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
        results = await free_ai_system.setup_all_providers()
        
        logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
        for provider, success in results.items():
            status = "‚úÖ" if success else "‚ùå"
            logger.info(f"   {status} {provider}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
        
        test_prompt = "–°–æ–∑–¥–∞–π –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"
        response = await free_ai_system.generate_response(test_prompt)
        logger.info(f"ü§ñ –û—Ç–≤–µ—Ç: {response}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        models = await free_ai_system.get_available_models()
        logger.info(f"üìã –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π: {models['total_models']}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
        status = await free_ai_system.get_model_status()
        logger.info(f"üìä –°—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤: {status}")
        
        logger.info("üéâ –°–∏—Å—Ç–µ–º–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π –≥–æ—Ç–æ–≤–∞!")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    asyncio.run(main())