#!/usr/bin/env python3
"""
Deployment Script for Enhanced Neural Network System
ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ² Ğ¾Ğ±Ğ»Ğ°ĞºĞµ
"""

import asyncio
import json
import logging
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class NeuralSystemDeployer:
    """Ğ Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
    
    def __init__(self):
        self.project_root = Path("/workspace")
        self.deployment_config = {
            "server_ip": "5.129.198.210",
            "server_port": 8080,
            "neural_port": 8081,
            "visual_port": 8082,
            "ollama_port": 11434
        }
        self.required_packages = [
            "torch",
            "transformers",
            "ollama",
            "fastapi",
            "uvicorn",
            "websockets",
            "pillow",
            "opencv-python",
            "numpy",
            "pandas",
            "scikit-learn",
            "matplotlib",
            "seaborn",
            "requests",
            "aiohttp",
            "asyncio",
            "schedule"
        ]
        self.services = []
        
    async def deploy_full_system(self):
        """ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
        try:
            logger.info("ğŸš€ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹...")
            
            # Ğ­Ñ‚Ğ°Ğ¿ 1: ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ÑÑ€ĞµĞ´Ñ‹
            await self._prepare_environment()
            
            # Ğ­Ñ‚Ğ°Ğ¿ 2: Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
            await self._install_dependencies()
            
            # Ğ­Ñ‚Ğ°Ğ¿ 3: ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ollama Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
            await self._setup_ollama()
            
            # Ğ­Ñ‚Ğ°Ğ¿ 4: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
            await self._create_services()
            
            # Ğ­Ñ‚Ğ°Ğ¿ 5: Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
            await self._start_system()
            
            # Ğ­Ñ‚Ğ°Ğ¿ 6: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ
            await self._health_check()
            
            # Ğ­Ñ‚Ğ°Ğ¿ 7: ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
            await self._setup_autostart()
            
            logger.info("âœ… Ğ Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾!")
            await self._print_deployment_summary()
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ: {e}")
            await self._cleanup_on_error()
            raise
    
    async def _prepare_environment(self):
        """ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ÑÑ€ĞµĞ´Ñ‹"""
        logger.info("ğŸ”§ ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ÑÑ€ĞµĞ´Ñ‹...")
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
        directories = [
            "/workspace/neural_data",
            "/workspace/neural_models",
            "/workspace/neural_logs",
            "/workspace/neural_cache",
            "/workspace/visual_screenshots",
            "/workspace/visual_reports"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ: {directory}")
        
        # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
        os.environ["NEURAL_SYSTEM_ROOT"] = str(self.project_root)
        os.environ["PYTHONPATH"] = f"{os.environ.get('PYTHONPATH', '')}:{self.project_root}"
        
        logger.info("âœ… Ğ¡Ñ€ĞµĞ´Ğ° Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ°")
    
    async def _install_dependencies(self):
        """Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹"""
        logger.info("ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹...")
        
        try:
            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ pip
            subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", "pip"], check=True)
            
            # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹
            for package in self.required_packages:
                try:
                    logger.info(f"ğŸ“¦ Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°Ñ {package}...")
                    subprocess.run([
                        sys.executable, "-m", "pip", "install", package, "--quiet"
                    ], check=True, timeout=300)
                    logger.info(f"âœ… {package} ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½")
                except subprocess.TimeoutExpired:
                    logger.warning(f"â° Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ {package}, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ...")
                except subprocess.CalledProcessError as e:
                    logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ {package}: {e}")
            
            logger.info("âœ… Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹")
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹: {e}")
            raise
    
    async def _setup_ollama(self):
        """ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ollama Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
        logger.info("ğŸ¤– ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ollama...")
        
        try:
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ»Ğ¸ Ollama
            try:
                result = subprocess.run(["ollama", "--version"], capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    logger.info(f"âœ… Ollama ÑƒĞ¶Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½: {result.stdout.strip()}")
                else:
                    await self._install_ollama()
            except FileNotFoundError:
                await self._install_ollama()
            
            # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ollama ÑĞµÑ€Ğ²ĞµÑ€
            await self._start_ollama_server()
            
            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            models_to_download = [
                "llama3.2:latest",
                "codellama:latest",
                "mistral:latest"
            ]
            
            for model in models_to_download:
                await self._download_model(model)
            
            logger.info("âœ… Ollama Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½")
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ollama: {e}")
            # ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ±ĞµĞ· Ollama, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ OpenAI API
            logger.info("âš ï¸ ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ±ĞµĞ· Ollama, Ğ±ÑƒĞ´ĞµÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ OpenAI API")
    
    async def _install_ollama(self):
        """Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ollama"""
        logger.info("ğŸ“¥ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ollama...")
        
        try:
            # Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ollama
            install_script = """
            curl -fsSL https://ollama.ai/install.sh | sh
            """
            
            process = subprocess.Popen(
                install_script,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(timeout=300)
            
            if process.returncode == 0:
                logger.info("âœ… Ollama ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾")
            else:
                logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ollama: {stderr}")
                raise Exception(f"Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ollama Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ: {stderr}")
                
        except subprocess.TimeoutExpired:
            logger.warning("â° Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ollama")
            raise Exception("Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ Ollama")
    
    async def _start_ollama_server(self):
        """Ğ—Ğ°Ğ¿ÑƒÑĞº Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ°"""
        logger.info("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ°...")
        
        try:
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ½Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ Ğ»Ğ¸ ÑƒĞ¶Ğµ ÑĞµÑ€Ğ²ĞµÑ€
            try:
                import requests
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    logger.info("âœ… Ollama ÑĞµÑ€Ğ²ĞµÑ€ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½")
                    return
            except:
                pass
            
            # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑĞµÑ€Ğ²ĞµÑ€ Ğ² Ñ„Ğ¾Ğ½Ğµ
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # Ğ–Ğ´ĞµĞ¼ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
            for i in range(30):
                try:
                    import requests
                    response = requests.get("http://localhost:11434/api/tags", timeout=2)
                    if response.status_code == 200:
                        logger.info("âœ… Ollama ÑĞµÑ€Ğ²ĞµÑ€ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½")
                        return
                except:
                    await asyncio.sleep(2)
            
            logger.warning("âš ï¸ Ollama ÑĞµÑ€Ğ²ĞµÑ€ Ğ½Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚, Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ±ĞµĞ· Ğ½ĞµĞ³Ğ¾")
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ°: {e}")
    
    async def _download_model(self, model: str):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ollama"""
        logger.info(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {model}...")
        
        try:
            process = subprocess.Popen(
                ["ollama", "pull", model],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Ğ–Ğ´ĞµĞ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ñ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ¾Ğ¼
            try:
                stdout, stderr = process.communicate(timeout=600)  # 10 Ğ¼Ğ¸Ğ½ÑƒÑ‚ Ğ½Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ
                
                if process.returncode == 0:
                    logger.info(f"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ {model} Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°")
                else:
                    logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ {model}: {stderr}")
                    
            except subprocess.TimeoutExpired:
                process.kill()
                logger.warning(f"â° Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {model}")
                
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ {model}: {e}")
    
    async def _create_services(self):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²"""
        logger.info("âš™ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²...")
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
        neural_service = self._create_neural_service()
        self.services.append(neural_service)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°
        visual_service = self._create_visual_service()
        self.services.append(visual_service)
        
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
        cloud_service = self._create_cloud_service()
        self.services.append(cloud_service)
        
        logger.info(f"âœ… Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(self.services)} ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²")
    
    def _create_neural_service(self) -> Dict[str, Any]:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑĞ° Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
        return {
            "name": "neural-system",
            "description": "Enhanced Neural Network System",
            "exec_start": f"{sys.executable} {self.project_root}/enhanced_neural_system.py",
            "working_directory": str(self.project_root),
            "environment": {
                "PYTHONPATH": str(self.project_root),
                "NEURAL_SYSTEM_ROOT": str(self.project_root)
            },
            "restart": "always",
            "port": self.deployment_config["neural_port"]
        }
    
    def _create_visual_service(self) -> Dict[str, Any]:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞµÑ€Ğ²Ğ¸ÑĞ° Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°"""
        return {
            "name": "visual-monitor",
            "description": "Visual Intelligence Monitor",
            "exec_start": f"{sys.executable} {self.project_root}/visual_monitor.py",
            "working_directory": str(self.project_root),
            "environment": {
                "PYTHONPATH": str(self.project_root),
                "DISPLAY": ":99"
            },
            "restart": "always",
            "port": self.deployment_config["visual_port"]
        }
    
    def _create_cloud_service(self) -> Dict[str, Any]:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°"""
        return {
            "name": "cloud-agent-system",
            "description": "Cloud Agent System",
            "exec_start": f"{sys.executable} {self.project_root}/cloud_agent_system.py",
            "working_directory": str(self.project_root),
            "environment": {
                "PYTHONPATH": str(self.project_root)
            },
            "restart": "always",
            "port": self.deployment_config["server_port"]
        }
    
    async def _start_system(self):
        """Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
        logger.info("ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹...")
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ
        for service in self.services:
            await self._start_service(service)
        
        logger.info("âœ… Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ°")
    
    async def _start_service(self, service: Dict[str, Any]):
        """Ğ—Ğ°Ğ¿ÑƒÑĞº ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°"""
        try:
            logger.info(f"ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº ÑĞµÑ€Ğ²Ğ¸ÑĞ° {service['name']}...")
            
            # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
            env = os.environ.copy()
            env.update(service.get("environment", {}))
            
            # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ
            process = subprocess.Popen(
                service["exec_start"].split(),
                cwd=service["working_directory"],
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            service["process"] = process
            service["pid"] = process.pid
            
            logger.info(f"âœ… Ğ¡ĞµÑ€Ğ²Ğ¸Ñ {service['name']} Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½ (PID: {process.pid})")
            
            # ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ğ¼Ğ¸
            await asyncio.sleep(2)
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿ÑƒÑĞºĞ° ÑĞµÑ€Ğ²Ğ¸ÑĞ° {service['name']}: {e}")
    
    async def _health_check(self):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
        logger.info("ğŸ’š ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹...")
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞµÑ€Ğ²Ğ¸Ñ
        healthy_services = 0
        
        for service in self.services:
            if await self._check_service_health(service):
                healthy_services += 1
        
        logger.info(f"ğŸ’š Ğ—Ğ´Ğ¾Ñ€Ğ¾Ğ²Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²: {healthy_services}/{len(self.services)}")
        
        if healthy_services == len(self.services):
            logger.info("âœ… Ğ’ÑĞµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾")
        else:
            logger.warning(f"âš ï¸ {len(self.services) - healthy_services} ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ² Ğ¸Ğ¼ĞµÑÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹")
    
    async def _check_service_health(self, service: Dict[str, Any]) -> bool:
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ğ²Ğ¸ÑĞ°"""
        try:
            process = service.get("process")
            if not process:
                return False
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ĞµÑ‰Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
            if process.poll() is not None:
                logger.warning(f"âš ï¸ Ğ¡ĞµÑ€Ğ²Ğ¸Ñ {service['name']} Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»ÑÑ")
                return False
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾ Ğ¿Ğ¾Ñ€Ñ‚Ñƒ (ĞµÑĞ»Ğ¸ ÑƒĞºĞ°Ğ·Ğ°Ğ½)
            port = service.get("port")
            if port:
                try:
                    import requests
                    response = requests.get(f"http://localhost:{port}/health", timeout=5)
                    if response.status_code == 200:
                        logger.info(f"âœ… Ğ¡ĞµÑ€Ğ²Ğ¸Ñ {service['name']} Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ¿Ğ¾Ñ€Ñ‚Ñƒ {port}")
                        return True
                except:
                    pass
            
            # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ñ€Ñ‚ Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½, ÑÑ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ñ‡Ñ‚Ğ¾ ÑĞµÑ€Ğ²Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞµÑĞ»Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ Ğ¶Ğ¸Ğ²Ğ¾Ğ¹
            logger.info(f"âœ… Ğ¡ĞµÑ€Ğ²Ğ¸Ñ {service['name']} Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ (PID: {process.pid})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ ÑĞµÑ€Ğ²Ğ¸ÑĞ° {service['name']}: {e}")
            return False
    
    async def _setup_autostart(self):
        """ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°"""
        logger.info("âš™ï¸ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°...")
        
        try:
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ startup ÑĞºÑ€Ğ¸Ğ¿Ñ‚
            startup_script = self._create_startup_script()
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞºÑ€Ğ¸Ğ¿Ñ‚
            script_path = self.project_root / "start_neural_system.sh"
            with open(script_path, "w") as f:
                f.write(startup_script)
            
            # Ğ”ĞµĞ»Ğ°ĞµĞ¼ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¸ÑĞ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼Ñ‹Ğ¼
            script_path.chmod(0o755)
            
            logger.info(f"âœ… Startup ÑĞºÑ€Ğ¸Ğ¿Ñ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {script_path}")
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ systemd ÑĞµÑ€Ğ²Ğ¸Ñ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°
            await self._create_systemd_service()
            
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°: {e}")
    
    def _create_startup_script(self) -> str:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ startup ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ğ°"""
        return f"""#!/bin/bash

# Enhanced Neural Network System Startup Script
# ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹

export PYTHONPATH="{self.project_root}:$PYTHONPATH"
export NEURAL_SYSTEM_ROOT="{self.project_root}"

cd {self.project_root}

echo "ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑĞº Enhanced Neural Network System..."

# Ğ—Ğ°Ğ¿ÑƒÑĞº Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ°
if command -v ollama &> /dev/null; then
    echo "ğŸ¤– Ğ—Ğ°Ğ¿ÑƒÑĞº Ollama ÑĞµÑ€Ğ²ĞµÑ€Ğ°..."
    ollama serve &
    sleep 5
fi

# Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ñ… ÑĞµÑ€Ğ²Ğ¸ÑĞ¾Ğ²
echo "ğŸ§  Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹..."
{sys.executable} enhanced_neural_system.py &

echo "ğŸ‘ï¸ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°..."
{sys.executable} visual_monitor.py &

echo "â˜ï¸ Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ¾Ğ±Ğ»Ğ°Ñ‡Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹..."
{sys.executable} cloud_agent_system.py &

echo "âœ… Ğ’ÑĞµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ñ‹"

# ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ
wait
"""
    
    async def _create_systemd_service(self):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ systemd ÑĞµÑ€Ğ²Ğ¸ÑĞ°"""
        try:
            service_content = f"""[Unit]
Description=Enhanced Neural Network System
After=network.target
Wants=network.target

[Service]
Type=forking
User=root
WorkingDirectory={self.project_root}
ExecStart={self.project_root}/start_neural_system.sh
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
"""
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑĞµÑ€Ğ²Ğ¸Ñ
            service_path = Path("/etc/systemd/system/neural-system.service")
            try:
                with open(service_path, "w") as f:
                    f.write(service_content)
                
                # ĞŸĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ systemd Ğ¸ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ ÑĞµÑ€Ğ²Ğ¸Ñ
                subprocess.run(["systemctl", "daemon-reload"], check=True)
                subprocess.run(["systemctl", "enable", "neural-system"], check=True)
                
                logger.info("âœ… Systemd ÑĞµÑ€Ğ²Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½")
                
            except PermissionError:
                logger.warning("âš ï¸ ĞĞµÑ‚ Ğ¿Ñ€Ğ°Ğ² Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ systemd ÑĞµÑ€Ğ²Ğ¸ÑĞ°")
                
        except Exception as e:
            logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ systemd ÑĞµÑ€Ğ²Ğ¸ÑĞ°: {e}")
    
    async def _cleanup_on_error(self):
        """ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞµ"""
        logger.info("ğŸ§¹ ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸...")
        
        # ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ·Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑÑ‹
        for service in self.services:
            process = service.get("process")
            if process and process.poll() is None:
                try:
                    process.terminate()
                    logger.info(f"ğŸ›‘ ĞÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ ÑĞµÑ€Ğ²Ğ¸Ñ {service['name']}")
                except:
                    pass
    
    async def _print_deployment_summary(self):
        """ĞŸĞµÑ‡Ğ°Ñ‚ÑŒ ÑĞ²Ğ¾Ğ´ĞºĞ¸ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ"""
        summary = f"""
ğŸ‰ Ğ ĞĞ—Ğ’Ğ•Ğ Ğ¢Ğ«Ğ’ĞĞĞ˜Ğ• Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ Ğ£Ğ¡ĞŸĞ•Ğ¨ĞĞ!

ğŸ“Š Ğ¡Ğ²Ğ¾Ğ´ĞºĞ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ§  Enhanced Neural Network System
   â”œâ”€â”€ ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°: http://localhost:{self.deployment_config['neural_port']}
   â”œâ”€â”€ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³: http://localhost:{self.deployment_config['visual_port']}
   â”œâ”€â”€ ĞĞ±Ğ»Ğ°Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°: http://localhost:{self.deployment_config['server_port']}
   â””â”€â”€ Ollama API: http://localhost:{self.deployment_config['ollama_port']}

ğŸš€ Ğ—Ğ°Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹:
"""
        
        for service in self.services:
            status = "ğŸŸ¢ Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚" if service.get("process") and service["process"].poll() is None else "ğŸ”´ ĞĞµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚"
            summary += f"   â”œâ”€â”€ {service['name']}: {status}\n"
        
        summary += f"""
ğŸ“ Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸:
   â”œâ”€â”€ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ: /workspace/neural_data
   â”œâ”€â”€ ĞœĞ¾Ğ´ĞµĞ»Ğ¸: /workspace/neural_models
   â”œâ”€â”€ Ğ›Ğ¾Ğ³Ğ¸: /workspace/neural_logs
   â”œâ”€â”€ ĞšÑÑˆ: /workspace/neural_cache
   â””â”€â”€ Ğ¡ĞºÑ€Ğ¸Ğ½ÑˆĞ¾Ñ‚Ñ‹: /workspace/visual_screenshots

ğŸ”§ Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹:
   â”œâ”€â”€ Ğ—Ğ°Ğ¿ÑƒÑĞº: ./start_neural_system.sh
   â”œâ”€â”€ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ: systemctl status neural-system
   â””â”€â”€ Ğ›Ğ¾Ğ³Ğ¸: journalctl -u neural-system -f

ğŸŒ Ğ”Ğ¾ÑÑ‚ÑƒĞ¿ Ğº ÑĞ¸ÑÑ‚ĞµĞ¼Ğµ:
   â”œâ”€â”€ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ: http://localhost:{self.deployment_config['server_port']}
   â”œâ”€â”€ ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°: http://localhost:{self.deployment_config['neural_port']}
   â””â”€â”€ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³: http://localhost:{self.deployment_config['visual_port']}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ¨ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ° Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ! ĞĞ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ½Ñ‹Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹.
ğŸ§  ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‚ÑÑ Ğ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒÑÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸.
ğŸ‘ï¸ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸.
â˜ï¸ ĞĞ±Ğ»Ğ°Ñ‡Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ 24/7 Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñƒ.

ğŸ¯ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ĞĞ’Ğ¢ĞĞĞĞœĞĞ Ğ¸ Ğ½Ğµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ!
"""
        
        print(summary)
        logger.info("ğŸ“‹ Ğ¡Ğ²Ğ¾Ğ´ĞºĞ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ Ğ²Ñ‹Ğ²ĞµĞ´ĞµĞ½Ğ°")

async def main():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ"""
    deployer = NeuralSystemDeployer()
    
    try:
        await deployer.deploy_full_system()
        
        # ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸
        logger.info("ğŸ’¤ Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ğ½ÑƒÑ‚Ğ°. ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Ctrl+C Ğ´Ğ»Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°...")
        while True:
            await asyncio.sleep(60)
            # ĞŸĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ
            await deployer._health_check()
            
    except KeyboardInterrupt:
        logger.info("ğŸ“¡ ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½ ÑĞ¸Ğ³Ğ½Ğ°Ğ» Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸")
        await deployer._cleanup_on_error()
    except Exception as e:
        logger.error(f"âŒ ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ñ: {e}")
        await deployer._cleanup_on_error()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())