#!/usr/bin/env python3
"""
–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π AI Engine –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏: Ollama, Hugging Face, –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã
"""

import asyncio
import json
import logging
import subprocess
import time
import requests
import os
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime

logger = logging.getLogger(__name__)

@dataclass
class AIResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç AI –º–æ–¥–µ–ª–∏"""
    content: str
    model: str
    provider: str
    tokens_used: int = 0
    response_time: float = 0.0
    success: bool = True
    error: Optional[str] = None

class OllamaEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏)"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.available_models = []
        self.free_models = [
            "llama3.1:8b",           # Meta Llama 3.1 8B
            "llama3.1:70b",          # Meta Llama 3.1 70B  
            "codellama:latest",      # Code Llama
            "mistral:latest",        # Mistral 7B
            "neural-chat:latest",    # Intel Neural Chat
            "starling-lm:latest",    # Starling LM
            "phi3:latest",           # Microsoft Phi-3
            "gemma:latest",          # Google Gemma
            "qwen:latest",           # Alibaba Qwen
            "deepseek-coder:latest", # DeepSeek Coder
            "tinyllama:latest",      # TinyLlama (–æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∞—è)
            "orca-mini:latest"       # Orca Mini
        ]
        self._load_models()
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                self.available_models = [line.split()[0] for line in lines if line.strip()]
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.available_models)} –º–æ–¥–µ–ª–µ–π Ollama")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Ollama")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π Ollama: {e}")
    
    async def install_free_models(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ Ollama...")
        
        for model in self.free_models:
            if model not in self.available_models:
                try:
                    logger.info(f"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {model}...")
                    process = subprocess.Popen(
                        ['ollama', 'pull', model],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True
                    )
                    
                    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    while process.poll() is None:
                        await asyncio.sleep(1)
                    
                    if process.returncode == 0:
                        logger.info(f"‚úÖ {model} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                        self.available_models.append(model)
                    else:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {model}")
                        
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {model}: {e}")
    
    async def generate_response(self, prompt: str, model: str = None, 
                              system_prompt: str = None, **kwargs) -> AIResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏"""
        start_time = time.time()
        
        if not model:
            model = self._get_best_available_model()
        
        if not model:
            return AIResponse(
                content="",
                model="none",
                provider="ollama",
                response_time=time.time() - start_time,
                success=False,
                error="–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π Ollama"
            )
        
        try:
            data = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "top_p": kwargs.get("top_p", 0.9),
                    "max_tokens": kwargs.get("max_tokens", 1000)
                }
            }
            
            if system_prompt:
                data["system"] = system_prompt
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=data,
                timeout=120  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
            )
            
            if response.status_code == 200:
                result = response.json()
                response_time = time.time() - start_time
                
                return AIResponse(
                    content=result.get("response", ""),
                    model=model,
                    provider="ollama",
                    tokens_used=len(result.get("response", "").split()),
                    response_time=response_time,
                    success=True
                )
            else:
                return AIResponse(
                    content="",
                    model=model,
                    provider="ollama",
                    response_time=time.time() - start_time,
                    success=False,
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except Exception as e:
            return AIResponse(
                content="",
                model=model,
                provider="ollama",
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _get_best_available_model(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –º–∞–ª–µ–Ω—å–∫–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        priority_models = ["tinyllama:latest", "orca-mini:latest", "phi3:latest", "mistral:latest"]
        
        for model in priority_models:
            if model in self.available_models:
                return model
        
        # –ï—Å–ª–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
        return self.available_models[0] if self.available_models else None
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False

class HuggingFaceEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Hugging Face (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏)"""
    
    def __init__(self):
        self.available_models = []
        self.free_models = [
            "microsoft/DialoGPT-medium",
            "distilbert-base-uncased", 
            "bert-base-uncased",
            "gpt2",
            "facebook/blenderbot-400M-distill",
            "microsoft/DialoGPT-small",
            "t5-small",
            "google/flan-t5-small"
        ]
        self._check_availability()
    
    def _check_availability(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Hugging Face"""
        try:
            import transformers
            self.available_models = self.free_models.copy()
            logger.info(f"‚úÖ Hugging Face –¥–æ—Å—Ç—É–ø–µ–Ω, {len(self.available_models)} –º–æ–¥–µ–ª–µ–π")
        except ImportError:
            logger.warning("‚ö†Ô∏è Hugging Face –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    async def generate_response(self, prompt: str, model: str = None, **kwargs) -> AIResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏"""
        start_time = time.time()
        
        if not self.available_models:
            return AIResponse(
                content="",
                model="none",
                provider="huggingface",
                response_time=time.time() - start_time,
                success=False,
                error="Hugging Face –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω"
            )
        
        if not model:
            model = "gpt2"  # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        try:
            from transformers import pipeline
            
            # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
            generator = pipeline(
                "text-generation",
                model=model,
                max_length=kwargs.get("max_tokens", 100),
                temperature=kwargs.get("temperature", 0.7),
                do_sample=True
            )
            
            result = generator(prompt, max_length=len(prompt.split()) + kwargs.get("max_tokens", 50))
            content = result[0]["generated_text"]
            
            response_time = time.time() - start_time
            
            return AIResponse(
                content=content,
                model=model,
                provider="huggingface",
                tokens_used=len(content.split()),
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            return AIResponse(
                content="",
                model=model,
                provider="huggingface",
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Hugging Face"""
        return len(self.available_models) > 0

class LocalTransformersEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
    
    def __init__(self):
        self.available_models = [
            "simple_classifier",
            "simple_generator", 
            "simple_analyzer",
            "simple_optimizer"
        ]
        logger.info(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–æ—Å—Ç—É–ø–Ω—ã, {len(self.available_models)} –º–æ–¥–µ–ª–µ–π")
    
    async def generate_response(self, prompt: str, model: str = None, **kwargs) -> AIResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        start_time = time.time()
        
        if not model:
            model = "simple_generator"
        
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            if model == "simple_classifier":
                content = f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {prompt[:50]}... (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)"
            elif model == "simple_generator":
                content = f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ: {prompt[:30]}... (–ª–æ–∫–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞)"
            elif model == "simple_analyzer":
                content = f"–ê–Ω–∞–ª–∏–∑: {prompt[:40]}... (–ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö)"
            elif model == "simple_optimizer":
                content = f"–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: {prompt[:35]}... (–ª–æ–∫–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)"
            else:
                content = f"–û—Ç–≤–µ—Ç –æ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ {model}: {prompt[:50]}..."
            
            response_time = time.time() - start_time
            
            return AIResponse(
                content=content,
                model=model,
                provider="local_transformers",
                tokens_used=len(content.split()),
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            return AIResponse(
                content="",
                model=model,
                provider="local_transformers",
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def is_available(self) -> bool:
        """–õ–æ–∫–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω—ã"""
        return True

class FreeAIEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–µ—Å–ø–ª–∞—Ç–Ω—ã–º–∏ AI –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self):
        self.ollama = OllamaEngine()
        self.huggingface = HuggingFaceEngine()
        self.local_transformers = LocalTransformersEngine()
        self.default_engine = None
        self._select_default_engine()
    
    def _select_default_engine(self):
        """–í—ã–±–æ—Ä –¥–≤–∏–∂–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        if self.ollama.is_available():
            self.default_engine = self.ollama
            logger.info("ü§ñ –í—ã–±—Ä–∞–Ω Ollama –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ AI")
        elif self.huggingface.is_available():
            self.default_engine = self.huggingface
            logger.info("ü§ñ –í—ã–±—Ä–∞–Ω Hugging Face –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ AI")
        else:
            self.default_engine = self.local_transformers
            logger.info("ü§ñ –í—ã–±—Ä–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ AI")
    
    async def setup_free_models(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π...")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏ Ollama
        if self.ollama.is_available():
            await self.ollama.install_free_models()
        
        logger.info("‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ AI –º–æ–¥–µ–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
    
    async def generate_response(self, prompt: str, model: str = None,
                              system_prompt: str = None, provider: str = None, **kwargs) -> AIResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI"""
        if not self.default_engine:
            return AIResponse(
                content="–ò–∑–≤–∏–Ω–∏—Ç–µ, AI –¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                model="none",
                provider="none",
                success=False,
                error="–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI –¥–≤–∏–∂–∫–æ–≤"
            )
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        if provider == "ollama" and self.ollama.is_available():
            selected_engine = self.ollama
        elif provider == "huggingface" and self.huggingface.is_available():
            selected_engine = self.huggingface
        elif provider == "local_transformers":
            selected_engine = self.local_transformers
        else:
            selected_engine = self.default_engine
        
        return await selected_engine.generate_response(
            prompt=prompt,
            model=model,
            system_prompt=system_prompt,
            **kwargs
        )
    
    def get_available_models(self) -> Dict[str, List[str]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        models = {}
        
        if self.ollama.is_available():
            models["ollama"] = self.ollama.available_models
        
        if self.huggingface.is_available():
            models["huggingface"] = self.huggingface.available_models
        
        if self.local_transformers.is_available():
            models["local_transformers"] = self.local_transformers.available_models
        
        return models
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å AI –¥–≤–∏–∂–∫–æ–≤"""
        return {
            "ollama_available": self.ollama.is_available(),
            "huggingface_available": self.huggingface.is_available(),
            "local_transformers_available": self.local_transformers.is_available(),
            "default_engine": self.default_engine.__class__.__name__ if self.default_engine else "none",
            "available_models": self.get_available_models()
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ AI –¥–≤–∏–∂–∫–∞
free_ai_engine = FreeAIEngine()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def generate_ai_response(prompt: str, system_prompt: str = None, **kwargs) -> str:
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI"""
    response = await free_ai_engine.generate_response(prompt, system_prompt=system_prompt, **kwargs)
    return response.content if response.success else f"–û—à–∏–±–∫–∞: {response.error}"

async def generate_code(prompt: str, language: str = "python") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞"""
    system_prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç. –°–æ–∑–¥–∞–≤–∞–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, —Ä–∞–±–æ—á–∏–π –∫–æ–¥ –Ω–∞ {language}.
–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –∫–æ–¥–æ–º –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å—è—Ç –∏–Ω–∞—á–µ."""
    
    response = await free_ai_engine.generate_response(prompt, system_prompt=system_prompt)
    return response.content if response.success else f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞: {response.error}"

async def analyze_data(prompt: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ, –Ω–∞—Ö–æ–¥–∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏,
—Å–æ–∑–¥–∞–≤–∞–π –∏–Ω—Å–∞–π—Ç—ã –∏ –¥–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    
    response = await free_ai_engine.generate_response(prompt, system_prompt=system_prompt)
    return response.content if response.success else f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {response.error}"

async def plan_project(prompt: str) -> str:
    """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"""
    system_prompt = """–¢—ã –æ–ø—ã—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤. –°–æ–∑–¥–∞–≤–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø–ª–∞–Ω—ã –ø—Ä–æ–µ–∫—Ç–æ–≤,
—Ä–∞–∑–±–∏–≤–∞–π –∑–∞–¥–∞—á–∏ –Ω–∞ —ç—Ç–∞–ø—ã, –æ—Ü–µ–Ω–∏–≤–∞–π —Ä–∏—Å–∫–∏ –∏ —Ä–µ—Å—É—Ä—Å—ã."""
    
    response = await free_ai_engine.generate_response(prompt, system_prompt=system_prompt)
    return response.content if response.success else f"–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {response.error}"

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ AI –¥–≤–∏–∂–∫–∞
    async def test_free_ai():
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ AI –¥–≤–∏–∂–∫–∞...")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏
        await free_ai_engine.setup_free_models()
        
        status = free_ai_engine.get_status()
        print(f"–°—Ç–∞—Ç—É—Å: {status}")
        
        if status["default_engine"] != "none":
            response = await generate_ai_response("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?")
            print(f"–û—Ç–≤–µ—Ç AI: {response}")
        else:
            print("‚ùå AI –¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    asyncio.run(test_free_ai())