#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π
–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä
"""

import asyncio
import json
import logging
import subprocess
import time
import requests
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)

class AutoModelInstaller:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.installation_log = []
        self.installed_models = {}
        self.failed_installations = []
        self._setup_directories()
    
    def _setup_directories(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –º–æ–¥–µ–ª–µ–π"""
        directories = [
            "/workspace/free_models",
            "/workspace/free_models/ollama",
            "/workspace/free_models/huggingface", 
            "/workspace/free_models/transformers",
            "/workspace/free_models/cache",
            "/workspace/free_models/logs"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
    
    async def install_ollama(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama"""
        try:
            logger.info("üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é Ollama...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏ —É–∂–µ Ollama
            result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                logger.info("‚úÖ Ollama —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                return True
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama
            install_script = """
            curl -fsSL https://ollama.ai/install.sh | sh
            """
            
            process = subprocess.Popen(
                install_script,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate()
            
            if process.returncode == 0:
                logger.info("‚úÖ Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                self.installation_log.append({
                    "component": "ollama",
                    "status": "installed",
                    "timestamp": datetime.now().isoformat()
                })
                return True
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {stderr}")
                self.failed_installations.append({
                    "component": "ollama",
                    "error": stderr,
                    "timestamp": datetime.now().isoformat()
                })
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {e}")
            self.failed_installations.append({
                "component": "ollama",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            return False
    
    async def start_ollama_server(self):
        """–ó–∞–ø—É—Å–∫ Ollama —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–µ—Ä–≤–µ—Ä
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    logger.info("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
                    return True
            except:
                pass
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä –≤ —Ñ–æ–Ω–µ
            process = subprocess.Popen(
                ['ollama', 'serve'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            )
            
            # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞
            for i in range(30):  # –ñ–¥–µ–º –¥–æ 30 —Å–µ–∫—É–Ω–¥
                try:
                    response = requests.get("http://localhost:11434/api/tags", timeout=2)
                    if response.status_code == 200:
                        logger.info("‚úÖ Ollama —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω")
                        return True
                except:
                    await asyncio.sleep(1)
            
            logger.warning("‚ö†Ô∏è Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è")
            return False
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ Ollama —Å–µ—Ä–≤–µ—Ä–∞: {e}")
            return False
    
    async def install_ollama_models(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π Ollama"""
        ollama_models = [
            {
                "name": "tinyllama:latest",
                "description": "TinyLlama - –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å",
                "size": "1.1GB",
                "priority": 1
            },
            {
                "name": "orca-mini:latest", 
                "description": "Orca Mini - –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤",
                "size": "1.9GB",
                "priority": 2
            },
            {
                "name": "phi3:latest",
                "description": "Microsoft Phi-3 - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å",
                "size": "2.3GB", 
                "priority": 3
            },
            {
                "name": "mistral:latest",
                "description": "Mistral 7B - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å",
                "size": "4.1GB",
                "priority": 4
            },
            {
                "name": "llama3.1:8b",
                "description": "Meta Llama 3.1 8B - –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å",
                "size": "4.7GB",
                "priority": 5
            },
            {
                "name": "codellama:latest",
                "description": "Code Llama - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è",
                "size": "3.8GB",
                "priority": 6
            }
        ]
        
        logger.info(f"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {len(ollama_models)} –º–æ–¥–µ–ª–µ–π Ollama...")
        
        for model_info in ollama_models:
            model_name = model_info["name"]
            try:
                logger.info(f"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {model_name} ({model_info['size']})...")
                
                process = subprocess.Popen(
                    ['ollama', 'pull', model_name],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                while process.poll() is None:
                    await asyncio.sleep(1)
                
                if process.returncode == 0:
                    logger.info(f"‚úÖ {model_name} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
                    self.installed_models[model_name] = {
                        "provider": "ollama",
                        "status": "installed",
                        "size": model_info["size"],
                        "installed_at": datetime.now().isoformat()
                    }
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {model_name}")
                    self.failed_installations.append({
                        "model": model_name,
                        "error": "Installation failed",
                        "timestamp": datetime.now().isoformat()
                    })
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {model_name}: {e}")
                self.failed_installations.append({
                    "model": model_name,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
    
    async def install_python_packages(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Python –ø–∞–∫–µ—Ç–æ–≤ –¥–ª—è AI"""
        packages = [
            "torch",
            "transformers",
            "accelerate",
            "sentencepiece",
            "protobuf",
            "numpy",
            "requests",
            "aiohttp",
            "aiohttp-cors"
        ]
        
        logger.info("üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é Python –ø–∞–∫–µ—Ç—ã –¥–ª—è AI...")
        
        for package in packages:
            try:
                logger.info(f"üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é {package}...")
                
                process = subprocess.Popen(
                    ['pip', 'install', package],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                stdout, stderr = process.communicate()
                
                if process.returncode == 0:
                    logger.info(f"‚úÖ {package} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {package}: {stderr}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {package}: {e}")
    
    async def install_huggingface_models(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π Hugging Face"""
        hf_models = [
            {
                "name": "gpt2",
                "description": "GPT-2 - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
                "size": "500MB"
            },
            {
                "name": "distilbert-base-uncased",
                "description": "DistilBERT - –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π BERT",
                "size": "250MB"
            },
            {
                "name": "t5-small",
                "description": "T5 Small - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å",
                "size": "240MB"
            },
            {
                "name": "google/flan-t5-small",
                "description": "FLAN-T5 Small - –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö",
                "size": "240MB"
            }
        ]
        
        logger.info(f"üì• –ö—ç—à–∏—Ä—É—é {len(hf_models)} –º–æ–¥–µ–ª–µ–π Hugging Face...")
        
        for model_info in hf_models:
            model_name = model_info["name"]
            try:
                logger.info(f"üì• –ö—ç—à–∏—Ä—É—é {model_name}...")
                
                # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∫—ç—à–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
                from transformers import AutoTokenizer, AutoModel
                
                tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    cache_dir="/workspace/free_models/huggingface"
                )
                model = AutoModel.from_pretrained(
                    model_name,
                    cache_dir="/workspace/free_models/huggingface"
                )
                
                logger.info(f"‚úÖ {model_name} –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∞")
                self.installed_models[model_name] = {
                    "provider": "huggingface",
                    "status": "cached",
                    "size": model_info["size"],
                    "cached_at": datetime.now().isoformat()
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è {model_name}: {e}")
                self.failed_installations.append({
                    "model": model_name,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
    
    async def create_local_models(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        local_models = [
            {
                "name": "simple_classifier",
                "description": "–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä",
                "type": "classification"
            },
            {
                "name": "simple_generator",
                "description": "–ü—Ä–æ—Å—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞",
                "type": "generation"
            },
            {
                "name": "simple_analyzer",
                "description": "–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö",
                "type": "analysis"
            },
            {
                "name": "simple_optimizer",
                "description": "–ü—Ä–æ—Å—Ç–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä",
                "type": "optimization"
            }
        ]
        
        logger.info(f"üîß –°–æ–∑–¥–∞—é {len(local_models)} –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π...")
        
        for model_info in local_models:
            model_name = model_info["name"]
            try:
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                model_path = f"/workspace/free_models/transformers/{model_name}.json"
                model_config = {
                    "name": model_name,
                    "description": model_info["description"],
                    "type": model_info["type"],
                    "created_at": datetime.now().isoformat(),
                    "status": "available"
                }
                
                with open(model_path, 'w', encoding='utf-8') as f:
                    json.dump(model_config, f, indent=2, ensure_ascii=False)
                
                logger.info(f"‚úÖ {model_name} —Å–æ–∑–¥–∞–Ω–∞")
                self.installed_models[model_name] = {
                    "provider": "local_transformers",
                    "status": "created",
                    "created_at": datetime.now().isoformat()
                }
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è {model_name}: {e}")
    
    async def install_all_models(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —É—Å—Ç–∞–Ω–æ–≤–∫—É –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π...")
        start_time = time.time()
        
        # 1. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Python –ø–∞–∫–µ—Ç—ã
        await self.install_python_packages()
        
        # 2. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Ollama
        ollama_installed = await self.install_ollama()
        
        if ollama_installed:
            # 3. –ó–∞–ø—É—Å–∫–∞–µ–º Ollama —Å–µ—Ä–≤–µ—Ä
            await self.start_ollama_server()
            
            # 4. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏ Ollama
            await self.install_ollama_models()
        
        # 5. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª–∏ Hugging Face
        try:
            await self.install_huggingface_models()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Hugging Face –º–æ–¥–µ–ª–µ–π: {e}")
        
        # 6. –°–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        await self.create_local_models()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        await self._save_installation_results()
        
        installation_time = time.time() - start_time
        
        logger.info("üéâ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {installation_time:.1f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(self.installed_models)}")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {len(self.failed_installations)}")
        
        return {
            "success": True,
            "installed_models": len(self.installed_models),
            "failed_installations": len(self.failed_installations),
            "installation_time": installation_time,
            "models": self.installed_models
        }
    
    async def _save_installation_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —É—Å—Ç–∞–Ω–æ–≤–∫–∏"""
        try:
            results = {
                "installation_log": self.installation_log,
                "installed_models": self.installed_models,
                "failed_installations": self.failed_installations,
                "total_installed": len(self.installed_models),
                "total_failed": len(self.failed_installations),
                "installation_time": datetime.now().isoformat()
            }
            
            results_path = "/workspace/free_models/logs/installation_results.json"
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
    
    async def get_installation_status(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏"""
        return {
            "installed_models": self.installed_models,
            "failed_installations": self.failed_installations,
            "total_installed": len(self.installed_models),
            "total_failed": len(self.failed_installations),
            "installation_log": self.installation_log
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫–∞
model_installer = AutoModelInstaller()

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        logger.info("üöÄ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö AI –º–æ–¥–µ–ª–µ–π")
        logger.info("=" * 60)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        results = await model_installer.install_all_models()
        
        logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –£–°–¢–ê–ù–û–í–ö–ò:")
        logger.info(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {results['installed_models']}")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {results['failed_installations']}")
        logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {results['installation_time']:.1f} —Å–µ–∫")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        logger.info("üìã –£–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ú–û–î–ï–õ–ò:")
        for model_name, model_info in results['models'].items():
            provider = model_info['provider']
            status = model_info['status']
            logger.info(f"   ‚Ä¢ {model_name} ({provider}) - {status}")
        
        logger.info("üéâ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    asyncio.run(main())