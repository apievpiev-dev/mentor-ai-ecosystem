#!/usr/bin/env python3
"""
–ê–≥–µ–Ω—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä AI –º–æ–¥–µ–ª–µ–π
–ú–æ–∂–µ—Ç —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å, —É–ø—Ä–∞–≤–ª—è—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å AI –º–æ–¥–µ–ª–∏
"""

import asyncio
import json
import logging
import subprocess
import time
import requests
import os
import uuid
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime

from multi_agent_system import BaseAgent, AgentType
from ai_engine import ai_engine, generate_ai_response

logger = logging.getLogger(__name__)

@dataclass
class ModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    name: str
    size: str
    status: str  # available, downloading, installed, error
    download_progress: float = 0.0
    last_used: Optional[str] = None
    performance_score: float = 0.0
    memory_usage: float = 0.0

class AIManagerAgent(BaseAgent):
    """–ê–≥–µ–Ω—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä AI –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, agent_id: str = None):
        super().__init__(
            agent_id or str(uuid.uuid4()),
            AgentType.SYSTEM_ADMIN,
            "AI –ú–µ–Ω–µ–¥–∂–µ—Ä",
            "–£–ø—Ä–∞–≤–ª—è–µ—Ç AI –º–æ–¥–µ–ª—è–º–∏, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –∏—Ö —Ä–∞–±–æ—Ç—É"
        )
        self.models = {}
        self.installation_queue = []
        self.performance_metrics = {}
        self._setup_skills()
        self._load_models_info()
    
    def _setup_skills(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞–≤—ã–∫–æ–≤ –∞–≥–µ–Ω—Ç–∞"""
        self.add_skill("install_model", self._handle_install_model)
        self.add_skill("list_models", self._handle_list_models)
        self.add_skill("optimize_models", self._handle_optimize_models)
        self.add_skill("monitor_performance", self._handle_monitor_performance)
        self.add_skill("cleanup_models", self._handle_cleanup_models)
        self.add_skill("setup_ai_environment", self._handle_setup_ai_environment)
    
    def _load_models_info(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                for line in lines:
                    if line.strip():
                        parts = line.split()
                        if len(parts) >= 3:
                            name = parts[0]
                            size = parts[1]
                            self.models[name] = ModelInfo(
                                name=name,
                                size=size,
                                status="installed",
                                last_used=datetime.now().isoformat()
                            )
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {len(self.models)} –º–æ–¥–µ–ª—è—Ö")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö: {e}")
    
    async def _handle_install_model(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        model_name = content.get("model_name", "")
        if not model_name:
            return {"error": "–ù–µ —É–∫–∞–∑–∞–Ω–æ –∏–º—è –º–æ–¥–µ–ª–∏"}
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏ —É–∂–µ –º–æ–¥–µ–ª—å
            if model_name in self.models:
                return {"message": f"–ú–æ–¥–µ–ª—å {model_name} —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞"}
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å —É—Å—Ç–∞–Ω–æ–≤–∫–∏
            self.installation_queue.append(model_name)
            self.models[model_name] = ModelInfo(
                name=model_name,
                size="unknown",
                status="downloading"
            )
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É –≤ —Ñ–æ–Ω–µ
            asyncio.create_task(self._install_model_async(model_name))
            
            return {
                "message": f"–ù–∞—á–∏–Ω–∞—é —É—Å—Ç–∞–Ω–æ–≤–∫—É –º–æ–¥–µ–ª–∏ {model_name}",
                "status": "downloading",
                "queue_position": len(self.installation_queue)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {str(e)}"}
    
    async def _install_model_async(self, model_name: str):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"üì• –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –º–æ–¥–µ–ª—å {model_name}...")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É
            process = subprocess.Popen(
                ['ollama', 'pull', model_name],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            while process.poll() is None:
                await asyncio.sleep(1)
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–∞—Ä—Å–∏—Ç—å –≤—ã–≤–æ–¥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            
            if process.returncode == 0:
                self.models[model_name].status = "installed"
                self.models[model_name].last_used = datetime.now().isoformat()
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            else:
                self.models[model_name].status = "error"
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}")
            
            # –£–¥–∞–ª—è–µ–º –∏–∑ –æ—á–µ—Ä–µ–¥–∏
            if model_name in self.installation_queue:
                self.installation_queue.remove(model_name)
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {model_name}: {e}")
            self.models[model_name].status = "error"
    
    async def _handle_list_models(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π"""
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
            self._load_models_info()
            
            models_info = []
            for model in self.models.values():
                models_info.append({
                    "name": model.name,
                    "size": model.size,
                    "status": model.status,
                    "last_used": model.last_used,
                    "performance_score": model.performance_score
                })
            
            return {
                "models": models_info,
                "total_models": len(models_info),
                "installing": len(self.installation_queue),
                "available_engines": ai_engine.get_status()
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            return {"error": str(e)}
    
    async def _handle_optimize_models(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
        try:
            optimization_results = []
            
            for model_name, model_info in self.models.items():
                if model_info.status == "installed":
                    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
                    start_time = time.time()
                    response = await ai_engine.generate_response(
                        "–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
                        model=model_name,
                        max_tokens=100
                    )
                    response_time = time.time() - start_time
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    model_info.performance_score = 1.0 / max(response_time, 0.1)
                    model_info.last_used = datetime.now().isoformat()
                    
                    optimization_results.append({
                        "model": model_name,
                        "response_time": response_time,
                        "performance_score": model_info.performance_score,
                        "success": response.success
                    })
            
            return {
                "message": "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                "results": optimization_results
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return {"error": str(e)}
    
    async def _handle_monitor_performance(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        try:
            performance_data = {
                "timestamp": datetime.now().isoformat(),
                "models": {},
                "system": {
                    "memory_usage": self._get_memory_usage(),
                    "cpu_usage": self._get_cpu_usage(),
                    "disk_usage": self._get_disk_usage()
                }
            }
            
            for model_name, model_info in self.models.items():
                performance_data["models"][model_name] = {
                    "status": model_info.status,
                    "performance_score": model_info.performance_score,
                    "last_used": model_info.last_used,
                    "memory_usage": model_info.memory_usage
                }
            
            return performance_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return {"error": str(e)}
    
    async def _handle_cleanup_models(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """–û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            cleanup_threshold = content.get("days_unused", 30)
            cutoff_date = datetime.now().timestamp() - (cleanup_threshold * 24 * 3600)
            
            models_to_remove = []
            for model_name, model_info in self.models.items():
                if model_info.last_used:
                    last_used_timestamp = datetime.fromisoformat(model_info.last_used).timestamp()
                    if last_used_timestamp < cutoff_date:
                        models_to_remove.append(model_name)
            
            removed_models = []
            for model_name in models_to_remove:
                try:
                    # –£–¥–∞–ª—è–µ–º –º–æ–¥–µ–ª—å
                    subprocess.run(['ollama', 'rm', model_name], check=True)
                    del self.models[model_name]
                    removed_models.append(model_name)
                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å: {model_name}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            
            return {
                "message": f"–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞",
                "removed_models": removed_models,
                "models_checked": len(self.models),
                "models_removed": len(removed_models)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return {"error": str(e)}
    
    async def _handle_setup_ai_environment(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ AI –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        try:
            setup_tasks = []
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏
            popular_models = [
                "llama3.1:8b",
                "llama3.1:70b", 
                "codellama:latest",
                "mistral:latest",
                "neural-chat:latest",
                "starling-lm:latest",
                "phi3:latest",
                "gemma:latest"
            ]
            
            for model in popular_models:
                if model not in self.models:
                    setup_tasks.append(self._install_model_async(model))
            
            if setup_tasks:
                # –ó–∞–ø—É—Å–∫–∞–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                await asyncio.gather(*setup_tasks, return_exceptions=True)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
            self._setup_environment()
            
            return {
                "message": "AI –æ–∫—Ä—É–∂–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ",
                "models_installing": len(setup_tasks),
                "total_models": len(self.models)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ AI –æ–∫—Ä—É–∂–µ–Ω–∏—è: {e}")
            return {"error": str(e)}
    
    def _setup_environment(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è AI"""
        try:
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è AI
            ai_dirs = [
                "/home/mentor/ai_models",
                "/home/mentor/ai_cache",
                "/home/mentor/ai_logs"
            ]
            
            for dir_path in ai_dirs:
                Path(dir_path).mkdir(parents=True, exist_ok=True)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            env_vars = {
                "OLLAMA_HOST": "0.0.0.0:11434",
                "OLLAMA_ORIGINS": "*",
                "OLLAMA_MODELS": "/home/mentor/ai_models"
            }
            
            for key, value in env_vars.items():
                os.environ[key] = value
            
            logger.info("‚úÖ AI –æ–∫—Ä—É–∂–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {e}")
    
    def _get_memory_usage(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
        try:
            result = subprocess.run(['free', '-m'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                mem_line = lines[1].split()
                used = int(mem_line[2])
                total = int(mem_line[1])
                return (used / total) * 100
        except:
            pass
        return 0.0
    
    def _get_cpu_usage(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU"""
        try:
            result = subprocess.run(['top', '-bn1'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                for line in lines:
                    if 'Cpu(s)' in line:
                        parts = line.split(',')
                        if len(parts) > 0:
                            cpu_part = parts[0].split()[1]
                            return float(cpu_part.replace('%', ''))
        except:
            pass
        return 0.0
    
    def _get_disk_usage(self) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏—Å–∫–∞"""
        try:
            result = subprocess.run(['df', '-h', '/'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                if len(lines) > 1:
                    parts = lines[1].split()
                    if len(parts) > 4:
                        usage = parts[4].replace('%', '')
                        return float(usage)
        except:
            pass
        return 0.0
    
    async def auto_install_models(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            available_models = await self._get_available_models()
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –º–æ–¥–µ–ª–∏
            for model in available_models:
                if model not in self.models:
                    await self._install_model_async(model)
                    await asyncio.sleep(5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —É—Å—Ç–∞–Ω–æ–≤–∫–∞–º–∏
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {e}")
    
    async def _get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        # –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        return [
            "llama3.1:8b",
            "llama3.1:70b",
            "codellama:latest",
            "mistral:latest",
            "neural-chat:latest",
            "starling-lm:latest",
            "phi3:latest",
            "gemma:latest",
            "qwen:latest",
            "deepseek-coder:latest"
        ]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–≥–µ–Ω—Ç–∞-–º–µ–Ω–µ–¥–∂–µ—Ä–∞ AI
ai_manager = AIManagerAgent()

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞-–º–µ–Ω–µ–¥–∂–µ—Ä–∞ AI
    async def test_ai_manager():
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI –º–µ–Ω–µ–¥–∂–µ—Ä–∞...")
        
        # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        result = await ai_manager._handle_list_models({})
        print(f"–ú–æ–¥–µ–ª–∏: {result}")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏
        result = await ai_manager._handle_install_model({"model_name": "phi3:latest"})
        print(f"–£—Å—Ç–∞–Ω–æ–≤–∫–∞: {result}")
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        result = await ai_manager._handle_monitor_performance({})
        print(f"–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {result}")
    
    asyncio.run(test_ai_manager())
