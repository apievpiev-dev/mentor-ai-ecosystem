#!/usr/bin/env python3
"""
AI Engine –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Ollama, OpenAI –∏ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
"""

import asyncio
import json
import logging
import requests
import subprocess
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class AIResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç AI –º–æ–¥–µ–ª–∏"""
    content: str
    model: str
    tokens_used: int = 0
    response_time: float = 0.0
    success: bool = True
    error: Optional[str] = None

class OllamaEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama"""
    
    def __init__(self, base_url: str = "http://localhost:11434", default_model: str = "llama3.2:latest"):
        self.base_url = base_url
        self.default_model = default_model
        self.available_models = []
        self.response_cache = {}  # –ö—ç—à –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        self.model_performance = {}  # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
        self.auto_model_selection = True  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        self._load_models()
        self._initialize_performance_tracking()
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                self.available_models = [line.split()[0] for line in lines if line.strip()]
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.available_models)} –º–æ–¥–µ–ª–µ–π Ollama")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Ollama")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π Ollama: {e}")
    
    def _initialize_performance_tracking(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
        for model in self.available_models:
            self.model_performance[model] = {
                "total_requests": 0,
                "successful_requests": 0,
                "average_response_time": 0.0,
                "average_tokens_per_second": 0.0,
                "error_rate": 0.0,
                "quality_score": 0.5  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            }
    
    def _select_best_model(self, prompt: str, **kwargs) -> str:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫"""
        if not self.auto_model_selection or not self.model_performance:
            return self.default_model
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        prompt_lower = prompt.lower()
        
        # –î–ª—è –∫–æ–¥–∞ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –ª—É—á—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
        if any(word in prompt_lower for word in ["–∫–æ–¥", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Ñ—É–Ω–∫—Ü–∏—è", "–∫–ª–∞—Å—Å", "debug"]):
            best_model = max(self.model_performance.keys(), 
                           key=lambda m: self.model_performance[m]["quality_score"])
        # –î–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º —Å–∫–æ—Ä–æ—Å—Ç—å
        elif len(prompt) < 100:
            best_model = min(self.model_performance.keys(), 
                           key=lambda m: self.model_performance[m]["average_response_time"] or float('inf'))
        else:
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: –∫–∞—á–µ—Å—Ç–≤–æ * —Å–∫–æ—Ä–æ—Å—Ç—å * –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å
            best_model = max(self.model_performance.keys(),
                           key=lambda m: (
                               self.model_performance[m]["quality_score"] * 
                               (1 / max(self.model_performance[m]["average_response_time"], 0.1)) *
                               (1 - self.model_performance[m]["error_rate"])
                           ))
        
        logger.info(f"ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {best_model}")
        return best_model
    
    def _update_model_performance(self, model: str, response: AIResponse):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        if model not in self.model_performance:
            self.model_performance[model] = {
                "total_requests": 0,
                "successful_requests": 0,
                "average_response_time": 0.0,
                "average_tokens_per_second": 0.0,
                "error_rate": 0.0,
                "quality_score": 0.5
            }
        
        metrics = self.model_performance[model]
        metrics["total_requests"] += 1
        
        if response.success:
            metrics["successful_requests"] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
            old_avg = metrics["average_response_time"]
            new_avg = (old_avg * (metrics["successful_requests"] - 1) + response.response_time) / metrics["successful_requests"]
            metrics["average_response_time"] = new_avg
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥—É
            if response.response_time > 0:
                tokens_per_sec = response.tokens_used / response.response_time
                old_tps = metrics["average_tokens_per_second"]
                new_tps = (old_tps * (metrics["successful_requests"] - 1) + tokens_per_sec) / metrics["successful_requests"]
                metrics["average_tokens_per_second"] = new_tps
            
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞ –∏ –≤—Ä–µ–º–µ–Ω–∏
            if len(response.content) > 50 and response.response_time < 30:
                metrics["quality_score"] = min(1.0, metrics["quality_score"] + 0.01)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ—à–∏–±–æ–∫
        metrics["error_rate"] = 1 - (metrics["successful_requests"] / metrics["total_requests"])
    
    async def generate_response(self, prompt: str, model: str = None, 
                              system_prompt: str = None, retry_count: int = 1, **kwargs) -> AIResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        start_time = time.time()
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if model is None:
            model = self._select_best_model(prompt, **kwargs)
        elif model not in self.available_models and self.available_models:
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –≤—ã–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
            model = self._select_best_model(prompt, **kwargs)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"{prompt[:100]}_{model}_{kwargs.get('temperature', 0.7)}_{kwargs.get('max_tokens', 1000)}"
        if cache_key in self.response_cache:
            logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç")
            cached_response = self.response_cache[cache_key]
            cached_response.response_time = 0.01  # –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞
            return cached_response
        
        for attempt in range(retry_count + 1):
            try:
                response = await self._make_request(prompt, model, system_prompt, start_time, **kwargs)
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                self._update_model_performance(model, response)
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ –∫—ç—à
                if response.success and response.content:
                    self.response_cache[cache_key] = response
                    logger.info("üíæ –û—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à")
                return response
            except requests.exceptions.Timeout:
                if attempt < retry_count:
                    logger.warning(f"‚è∞ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ—É–¥–∞—á–Ω–∞, –ø–æ–≤—Ç–æ—Ä—è–µ–º...")
                    await asyncio.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                    continue
                else:
                    response_time = time.time() - start_time
                    logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç AI –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {retry_count + 1} –ø–æ–ø—ã—Ç–æ–∫ ({response_time:.1f} —Å–µ–∫)")
                    return AIResponse(
                        content="",
                        model=model,
                        response_time=response_time,
                        success=False,
                        error="AI timeout - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –æ—Ç–≤–µ—Ç"
                    )
            except Exception as e:
                if attempt < retry_count:
                    logger.warning(f"‚ùå –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ—É–¥–∞—á–Ω–∞: {e}, –ø–æ–≤—Ç–æ—Ä—è–µ–º...")
                    await asyncio.sleep(1)
                    continue
                else:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ—Å–ª–µ {retry_count + 1} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                    return AIResponse(
                        content="",
                        model=model,
                        response_time=time.time() - start_time,
                        success=False,
                        error=str(e)
                    )
    
    async def _make_request(self, prompt: str, model: str, system_prompt: str, start_time: float, **kwargs) -> AIResponse:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama"""
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.5),  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
                "top_p": kwargs.get("top_p", 0.8),  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                "top_k": 20,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤
                "repeat_penalty": 1.1,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
                "num_ctx": 1024,  # –£–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
                "num_predict": kwargs.get("max_tokens", 100)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            }
        }
        
        if system_prompt:
            data["system"] = system_prompt
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º –¥–ª—è AI –æ—Ç–≤–µ—Ç–æ–≤
        response = requests.post(
            f"{self.base_url}/api/generate",
            json=data,
            timeout=60  # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
        )
        
        if response.status_code == 200:
            result = response.json()
            response_time = time.time() - start_time
            
            return AIResponse(
                content=result.get("response", ""),
                model=model,
                tokens_used=len(result.get("response", "").split()),
                response_time=response_time,
                success=True
            )
        else:
            return AIResponse(
                content="",
                model=model,
                response_time=time.time() - start_time,
                success=False,
                error=f"HTTP {response.status_code}: {response.text}"
            )
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –æ—Ç–≤–µ—Ç–æ–≤"""
        self.response_cache.clear()
        logger.info("üßπ –ö—ç—à –æ—Ç–≤–µ—Ç–æ–≤ –æ—á–∏—â–µ–Ω")
    
    def get_model_performance(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
        return self.model_performance.copy()
    
    def get_health_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–¥–æ—Ä–æ–≤—å—è AI –¥–≤–∏–∂–∫–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                models = response.json().get("models", [])
                return {
                    "status": "healthy",
                    "available_models": len(models),
                    "default_model": self.default_model,
                    "response_time": response.elapsed.total_seconds(),
                    "cache_size": len(self.response_cache),
                    "auto_model_selection": self.auto_model_selection,
                    "performance_metrics": self.model_performance
                }
            else:
                return {
                    "status": "unhealthy",
                    "error": f"HTTP {response.status_code}",
                    "available_models": 0,
                    "cache_size": len(self.response_cache)
                }
        except requests.exceptions.Timeout:
            return {
                "status": "timeout",
                "error": "Connection timeout",
                "available_models": 0,
                "cache_size": len(self.response_cache)
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "available_models": 0,
                "cache_size": len(self.response_cache)
            }

class OpenAIEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API"""
    
    def __init__(self, api_key: str = None, default_model: str = "gpt-3.5-turbo"):
        self.api_key = api_key or self._get_api_key()
        self.default_model = default_model
        self.base_url = "https://api.openai.com/v1"
    
    def _get_api_key(self) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        import os
        return os.getenv("OPENAI_API_KEY")
    
    async def generate_response(self, prompt: str, model: str = None,
                              system_prompt: str = None, **kwargs) -> AIResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç OpenAI"""
        if not self.api_key:
            return AIResponse(
                content="",
                model=model or self.default_model,
                success=False,
                error="OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω"
            )
        
        start_time = time.time()
        model = model or self.default_model
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            data = {
                "model": model,
                "messages": messages,
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 1000),
                "num_ctx": 1024,  # –£–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
                "num_predict": kwargs.get("max_tokens", 1000),  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                "repeat_penalty": 1.1,  # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä–æ–≤
                "top_k": 20,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤—ã–±–æ—Ä–∞ —Ç–æ–∫–µ–Ω–æ–≤
                "top_p": 0.9  # –Ø–¥–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=150  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                response_time = time.time() - start_time
                
                return AIResponse(
                    content=content,
                    model=model,
                    tokens_used=result.get("usage", {}).get("total_tokens", 0),
                    response_time=response_time,
                    success=True
                )
            else:
                return AIResponse(
                    content="",
                    model=model,
                    response_time=time.time() - start_time,
                    success=False,
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ OpenAI: {e}")
            return AIResponse(
                content="",
                model=model,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def is_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ OpenAI API"""
        return self.api_key is not None

class AIEngine:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å AI"""
    
    def __init__(self):
        self.ollama = OllamaEngine()
        self.openai = OpenAIEngine()
        self.default_engine = None
        self._select_default_engine()
    
    def _select_default_engine(self):
        """–í—ã–±–æ—Ä –¥–≤–∏–∂–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        if self.ollama.is_available():
            self.default_engine = self.ollama
            logger.info("ü§ñ –í—ã–±—Ä–∞–Ω Ollama –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ AI")
        elif self.openai.is_available():
            self.default_engine = self.openai
            logger.info("ü§ñ –í—ã–±—Ä–∞–Ω OpenAI –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–≤–∏–∂–æ–∫ AI")
        else:
            logger.warning("‚ö†Ô∏è –ù–∏ –æ–¥–∏–Ω AI –¥–≤–∏–∂–æ–∫ –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
    
    async def generate_response(self, prompt: str, model: str = None,
                              system_prompt: str = None, engine: str = None, **kwargs) -> AIResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI"""
        if not self.default_engine:
            return AIResponse(
                content="–ò–∑–≤–∏–Ω–∏—Ç–µ, AI –¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                model="none",
                success=False,
                error="–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI –¥–≤–∏–∂–∫–æ–≤"
            )
        
        # –í—ã–±–∏—Ä–∞–µ–º –¥–≤–∏–∂–æ–∫
        if engine == "ollama" and self.ollama.is_available():
            selected_engine = self.ollama
        elif engine == "openai" and self.openai.is_available():
            selected_engine = self.openai
        else:
            selected_engine = self.default_engine
        
        return await selected_engine.generate_response(
            prompt=prompt,
            model=model,
            system_prompt=system_prompt,
            **kwargs
        )
    
    def get_available_models(self) -> Dict[str, List[str]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        models = {}
        
        if self.ollama.is_available():
            models["ollama"] = self.ollama.available_models
        
        if self.openai.is_available():
            models["openai"] = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
        
        return models
    
    def get_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å AI –¥–≤–∏–∂–∫–æ–≤"""
        return {
            "ollama_available": self.ollama.is_available(),
            "openai_available": self.openai.is_available(),
            "default_engine": "ollama" if self.default_engine == self.ollama else "openai" if self.default_engine == self.openai else "none",
            "available_models": self.get_available_models()
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä AI –¥–≤–∏–∂–∫–∞
ai_engine = AIEngine()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def generate_ai_response(prompt: str, system_prompt: str = None, **kwargs) -> str:
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç AI"""
    response = await ai_engine.generate_response(prompt, system_prompt=system_prompt, **kwargs)
    return response.content if response.success else f"–û—à–∏–±–∫–∞: {response.error}"

async def generate_code(prompt: str, language: str = "python") -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞"""
    system_prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç. –°–æ–∑–¥–∞–≤–∞–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π, —Ä–∞–±–æ—á–∏–π –∫–æ–¥ –Ω–∞ {language}.
–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –∫–æ–¥–æ–º –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å—è—Ç –∏–Ω–∞—á–µ."""
    
    response = await ai_engine.generate_response(prompt, system_prompt=system_prompt)
    return response.content if response.success else f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞: {response.error}"

async def analyze_data(prompt: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
    system_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∞–Ω–Ω—ã—Ö. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ, –Ω–∞—Ö–æ–¥–∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏,
—Å–æ–∑–¥–∞–≤–∞–π –∏–Ω—Å–∞–π—Ç—ã –∏ –¥–∞–≤–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    
    response = await ai_engine.generate_response(prompt, system_prompt=system_prompt)
    return response.content if response.success else f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {response.error}"

async def plan_project(prompt: str) -> str:
    """–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞"""
    system_prompt = """–¢—ã –æ–ø—ã—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–µ–∫—Ç–æ–≤. –°–æ–∑–¥–∞–≤–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø–ª–∞–Ω—ã –ø—Ä–æ–µ–∫—Ç–æ–≤,
—Ä–∞–∑–±–∏–≤–∞–π –∑–∞–¥–∞—á–∏ –Ω–∞ —ç—Ç–∞–ø—ã, –æ—Ü–µ–Ω–∏–≤–∞–π —Ä–∏—Å–∫–∏ –∏ —Ä–µ—Å—É—Ä—Å—ã."""
    
    response = await ai_engine.generate_response(prompt, system_prompt=system_prompt)
    return response.content if response.success else f"–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {response.error}"

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI –¥–≤–∏–∂–∫–∞
    async def test_ai():
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AI –¥–≤–∏–∂–∫–∞...")
        
        status = ai_engine.get_status()
        print(f"–°—Ç–∞—Ç—É—Å: {status}")
        
        if status["default_engine"] != "none":
            response = await generate_ai_response("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?")
            print(f"–û—Ç–≤–µ—Ç AI: {response}")
        else:
            print("‚ùå AI –¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
    
    asyncio.run(test_ai())
